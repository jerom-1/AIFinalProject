{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T04:54:12.759458Z",
     "iopub.status.busy": "2025-06-01T04:54:12.759226Z",
     "iopub.status.idle": "2025-06-01T04:54:34.733867Z",
     "shell.execute_reply": "2025-06-01T04:54:34.733235Z",
     "shell.execute_reply.started": "2025-06-01T04:54:12.759436Z"
    },
    "papermill": {
     "duration": 44.581645,
     "end_time": "2025-02-12T16:07:49.023612",
     "exception": false,
     "start_time": "2025-02-12T16:07:04.441967",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/intel-image-classification/seg_train/seg_train/mountain/14986.jpg\n",
      "/kaggle/input/intel-image-classification/seg_train/seg_train/street/1269.jpg\n",
      "/kaggle/input/intel-image-classification/seg_train/seg_train/buildings/2193.jpg\n",
      "/kaggle/input/intel-image-classification/seg_train/seg_train/sea/19812.jpg\n",
      "/kaggle/input/intel-image-classification/seg_train/seg_train/forest/7981.jpg\n",
      "/kaggle/input/intel-image-classification/seg_train/seg_train/glacier/12666.jpg\n",
      "/kaggle/input/intel-image-classification/seg_pred/seg_pred/6234.jpg\n",
      "/kaggle/input/intel-image-classification/seg_test/seg_test/mountain/22608.jpg\n",
      "/kaggle/input/intel-image-classification/seg_test/seg_test/street/20088.jpg\n",
      "/kaggle/input/intel-image-classification/seg_test/seg_test/buildings/22735.jpg\n",
      "/kaggle/input/intel-image-classification/seg_test/seg_test/sea/20513.jpg\n",
      "/kaggle/input/intel-image-classification/seg_test/seg_test/forest/23407.jpg\n",
      "/kaggle/input/intel-image-classification/seg_test/seg_test/glacier/23149.jpg\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T04:55:04.382049Z",
     "iopub.status.busy": "2025-06-01T04:55:04.381522Z",
     "iopub.status.idle": "2025-06-01T04:55:12.188700Z",
     "shell.execute_reply": "2025-06-01T04:55:12.187915Z",
     "shell.execute_reply.started": "2025-06-01T04:55:04.382025Z"
    },
    "papermill": {
     "duration": 2.41988,
     "end_time": "2025-02-12T16:07:58.62654",
     "exception": false,
     "start_time": "2025-02-12T16:07:56.20666",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Import deep learning libraries\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split, Subset, Dataset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T04:55:40.724170Z",
     "iopub.status.busy": "2025-06-01T04:55:40.723443Z",
     "iopub.status.idle": "2025-06-01T04:55:40.728251Z",
     "shell.execute_reply": "2025-06-01T04:55:40.727424Z",
     "shell.execute_reply.started": "2025-06-01T04:55:40.724146Z"
    },
    "papermill": {
     "duration": 0.012806,
     "end_time": "2025-02-12T16:07:58.643809",
     "exception": false,
     "start_time": "2025-02-12T16:07:58.631003",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.Resize((150, 150)),                # Resize images to 150x150\n",
    "    transforms.ToTensor(),                        # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])   # Normalize images to [0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T04:55:43.658452Z",
     "iopub.status.busy": "2025-06-01T04:55:43.657866Z",
     "iopub.status.idle": "2025-06-01T04:55:48.848936Z",
     "shell.execute_reply": "2025-06-01T04:55:48.848196Z",
     "shell.execute_reply.started": "2025-06-01T04:55:43.658427Z"
    },
    "papermill": {
     "duration": 0.011325,
     "end_time": "2025-02-12T16:08:06.579304",
     "exception": false,
     "start_time": "2025-02-12T16:08:06.567979",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 150, 150]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "#train dataset\n",
    "train_path = '/kaggle/input/intel-image-classification/seg_train/seg_train'\n",
    "train_dataset = datasets.ImageFolder(root=train_path, transform=transform)\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Example: Iterate over the first batch\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T04:55:50.879519Z",
     "iopub.status.busy": "2025-06-01T04:55:50.879226Z",
     "iopub.status.idle": "2025-06-01T04:55:52.284564Z",
     "shell.execute_reply": "2025-06-01T04:55:52.283677Z",
     "shell.execute_reply.started": "2025-06-01T04:55:50.879468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Validation and test data\n",
    "val_path = '/kaggle/input/intel-image-classification/seg_test/seg_test'\n",
    "val_dataset_full = datasets.ImageFolder(root=val_path, transform=transform)\n",
    "\n",
    "targets = np.array(val_dataset_full.targets)  # Labels for stratification\n",
    "sample_size = 100  # Total sample size\n",
    "\n",
    "num_iterations = 2  # Number of different samples you want\n",
    "# Create stratified split with multiple iterations\n",
    "splitter = StratifiedShuffleSplit(n_splits=num_iterations, test_size=sample_size, random_state=42)\n",
    "\n",
    "sample_datasets = []\n",
    "for train_idx, sample_idx in splitter.split(np.zeros(len(targets)), targets):\n",
    "    sample_datasets.append(Subset(val_dataset_full, sample_idx))\n",
    "\n",
    "val_dataset = sample_datasets[0]\n",
    "test_dataset = sample_datasets[1]\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 100\n",
    "# Create DataLoaders\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Conv2D:\n",
    "    \"\"\"\n",
    "    2D Convolutional layer with kernel size 3x3, stride 1, padding 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Initialize weights and biases (He initialization)\n",
    "        scale = np.sqrt(2.0 / (in_channels * kernel_size * kernel_size))\n",
    "        self.weight = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * scale\n",
    "        self.bias = np.zeros(out_channels)\n",
    "\n",
    "        # Gradients placeholders\n",
    "        self.grad_weight = np.zeros_like(self.weight)\n",
    "        self.grad_bias = np.zeros_like(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: perform convolution.\n",
    "        x shape: (N, C_in, H, W)\n",
    "              Returns: out shape: (N, C_out, H_out, W_out)\n",
    "        \"\"\"\n",
    "        self.x = x  # Save input for backward\n",
    "        N, C, H, W = x.shape\n",
    "        assert C == self.in_channels, \"Input channel dimension mismatch.\"\n",
    "\n",
    "        # Pad input\n",
    "        pad = self.padding\n",
    "        x_padded = np.pad(x, ((0,0),(0,0),(pad,pad),(pad,pad)), mode='constant', constant_values=0)\n",
    "\n",
    "        # Compute output dimensions\n",
    "        out_H = (H + 2*pad - self.kernel_size) // self.stride + 1\n",
    "        out_W = (W + 2*pad - self.kernel_size) // self.stride + 1\n",
    "\n",
    "        # Initialize output tensor\n",
    "        out = np.zeros((N, self.out_channels, out_H, out_W))\n",
    "\n",
    "        # Perform convolution (naive loop implementation)\n",
    "        for n in range(N):\n",
    "            for o in range(self.out_channels):\n",
    "                for i in range(out_H):\n",
    "                    for j in range(out_W):\n",
    "                        i0 = i * self.stride\n",
    "                        j0 = j * self.stride\n",
    "                        region = x_padded[n, :, i0:i0+self.kernel_size, j0:j0+self.kernel_size]\n",
    "                        out[n, o, i, j] = np.sum(region * self.weight[o]) + self.bias[o]\n",
    "        self.out = out  # Save output (if needed)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients w.r.t. input, weights, biases.\n",
    "        dout shape: (N, C_out, H_out, W_out)\n",
    "        Returns: dx shape: (N, C_in, H, W)\n",
    "        \"\"\"\n",
    "        x = self.x\n",
    "        N, C_in, H, W = x.shape\n",
    "        pad = self.padding\n",
    "        x_padded = np.pad(x, ((0,0),(0,0),(pad,pad),(pad,pad)), mode='constant', constant_values=0)\n",
    "\n",
    "        # Initialize gradients\n",
    "        dx_padded = np.zeros_like(x_padded)\n",
    "        self.grad_weight = np.zeros_like(self.weight)\n",
    "        self.grad_bias = np.zeros_like(self.bias)\n",
    "\n",
    "        # Gradient w.r.t. bias: sum over batch, height and width for each output channel\n",
    "        self.grad_bias = np.sum(dout, axis=(0,2,3))\n",
    "\n",
    "        N, C_out, out_H, out_W = dout.shape\n",
    "        # Compute gradients w.r.t. weights and inputs\n",
    "        for n in range(N):\n",
    "            for o in range(C_out):\n",
    "                for i in range(out_H):\n",
    "                    for j in range(out_W):\n",
    "                        i0 = i * self.stride\n",
    "                        j0 = j * self.stride\n",
    "                        region = x_padded[n, :, i0:i0+self.kernel_size, j0:j0+self.kernel_size]\n",
    "                        # Weight gradient accumulation\n",
    "                        self.grad_weight[o] += region * dout[n, o, i, j]\n",
    "                        # Input gradient propagation\n",
    "                        dx_padded[n, :, i0:i0+self.kernel_size, j0:j0+self.kernel_size] += self.weight[o] * dout[n, o, i, j]\n",
    "\n",
    "        # Remove padding from dx\n",
    "        if pad > 0:\n",
    "            dx = dx_padded[:, :, pad:-pad, pad:-pad]\n",
    "        else:\n",
    "            dx = dx_padded\n",
    "        return dx\n",
    "\n",
    "\n",
    "class BatchNorm2D:\n",
    "    \"\"\"\n",
    "    Batch Normalization layer for 2D convolution outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, eps=1e-5):\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        # Scale and shift parameters\n",
    "        self.gamma = np.ones(num_features)\n",
    "        self.beta = np.zeros(num_features)\n",
    "        # Gradients\n",
    "        self.grad_gamma = np.zeros_like(self.gamma)\n",
    "        self.grad_beta = np.zeros_like(self.beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward batch norm.\n",
    "        x shape: (N, C, H, W)\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        N, C, H, W = x.shape\n",
    "        # Flatten spatial dimensions for computing mean/var per channel\n",
    "        x_flat = x.transpose(1, 0, 2, 3).reshape(C, -1)\n",
    "\n",
    "        # Compute mean and variance per channel\n",
    "        self.mean = np.mean(x_flat, axis=1)\n",
    "        self.var = np.var(x_flat, axis=1)\n",
    "        self.std = np.sqrt(self.var + self.eps)\n",
    "\n",
    "        # Normalize\n",
    "        x_norm_flat = (x_flat - self.mean[:, None]) / self.std[:, None]\n",
    "         # Reshape back to original\n",
    "        x_norm = x_norm_flat.reshape(C, N, H, W).transpose(1, 0, 2, 3)\n",
    "\n",
    "        # Scale and shift\n",
    "        out = self.gamma[None, :, None, None] * x_norm + self.beta[None, :, None, None]\n",
    "        self.x_norm = x_norm  # Save normalized input for backward\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass for batch norm.\n",
    "        dout shape: (N, C, H, W)\n",
    "        \"\"\"\n",
    "        N, C, H, W = dout.shape\n",
    "        # Reshape to (C, N*H*W)\n",
    "        dout_flat = dout.transpose(1, 0, 2, 3).reshape(C, -1)\n",
    "        x_flat = self.x.transpose(1, 0, 2, 3).reshape(C, -1)\n",
    "\n",
    "        # Gradients for gamma and beta\n",
    "        x_norm_flat = self.x_norm.transpose(1, 0, 2, 3).reshape(C, -1)\n",
    "        self.grad_gamma = np.sum(dout_flat * x_norm_flat, axis=1)\n",
    "        self.grad_beta = np.sum(dout_flat, axis=1)\n",
    "\n",
    "        # Gradient w.r.t. normalized input\n",
    "        dx_norm = (dout * self.gamma[None, :, None, None]).transpose(1, 0, 2, 3).reshape(C, -1)\n",
    "\n",
    "        # Intermediate terms for backward pass\n",
    "        m = N * H * W\n",
    "        x_mu = x_flat - self.mean[:, None]\n",
    "\n",
    "        # Gradients w.r.t. variance and mean\n",
    "        dvar = np.sum(dx_norm * x_mu * -0.5 * (self.var[:, None] + self.eps)**(-1.5), axis=1)\n",
    "        dmean = np.sum(dx_norm * -1.0 / self.std[:, None], axis=1) + dvar * np.mean(-2.0 * x_mu, axis=1)\n",
    "\n",
    "        # Gradient w.r.t. input\n",
    "        dx_flat = (dx_norm / self.std[:, None]) + (dvar[:, None] * 2 * x_mu / m) + (dmean[:, None] / m)\n",
    "        dx = dx_flat.reshape(C, N, H, W).transpose(1, 0, 2, 3)\n",
    "        return dx\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    \"\"\"\n",
    "    ReLU activation layer.\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        self.mask = (x > 0)\n",
    "        return x * self.mask\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "\n",
    "class MaxPool2D:\n",
    "    \"\"\"\n",
    "    Max pooling layer with kernel size 2x2 and stride 2.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size=2, stride=2):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (N, C, H, W)\n",
    "        Returns: pooled output of shape (N, C, H_out, W_out).\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        N, C, H, W = x.shape\n",
    "        pool = self.kernel_size\n",
    "        stride = self.stride\n",
    "        # Compute output dimensions\n",
    "        out_H = (H - pool) // stride + 1\n",
    "        out_W = (W - pool) // stride + 1\n",
    "\n",
    "        out = np.zeros((N, C, out_H, out_W))\n",
    "        self.mask = np.zeros_like(x)  # mask for max indices\n",
    "\n",
    "        for n in range(N):\n",
    "            for c in range(C):\n",
    "                for i in range(out_H):\n",
    "                    for j in range(out_W):\n",
    "                        i0 = i * stride\n",
    "                        j0 = j * stride\n",
    "                        region = x[n, c, i0:i0+pool, j0:j0+pool]\n",
    "                        max_val = np.max(region)\n",
    "                        out[n, c, i, j] = max_val\n",
    "                        # Mask the location of max value for backward\n",
    "                        max_idx = np.unravel_index(np.argmax(region), region.shape)\n",
    "                        self.mask[n, c, i0+max_idx[0], j0+max_idx[1]] = 1\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backpropagate through max pool.\n",
    "        dout shape: (N, C, H_out, W_out)\n",
    "        \"\"\"\n",
    "        N, C, out_H, out_W = dout.shape\n",
    "        pool = self.kernel_size\n",
    "        stride = self.stride\n",
    "        dx = np.zeros_like(self.x)\n",
    "\n",
    "        for n in range(N):\n",
    "            for c in range(C):\n",
    "                for i in range(out_H):\n",
    "                    for j in range(out_W):\n",
    "                        i0 = i * stride\n",
    "                        j0 = j * stride\n",
    "                        # Propagate gradient to the max location\n",
    "                        dx[n, c, i0:i0+pool, j0:j0+pool] += \\\n",
    "                            self.mask[n, c, i0:i0+pool, j0:j0+pool] * dout[n, c, i, j]\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Flatten:\n",
    "    \"\"\"\n",
    "    Flatten layer to convert 4D tensor to 2D.\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        self.orig_shape = x.shape\n",
    "        N = x.shape[0]\n",
    "        return x.reshape(N, -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout.reshape(self.orig_shape)\n",
    "\n",
    "\n",
    "class Linear:\n",
    "    \"\"\"\n",
    "    Fully connected linear layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # Xavier/He initialization\n",
    "        scale = np.sqrt(2.0 / in_features)\n",
    "        self.weight = np.random.randn(in_features, out_features) * scale\n",
    "        self.bias = np.zeros(out_features)\n",
    "        self.grad_weight = np.zeros_like(self.weight)\n",
    "        self.grad_bias = np.zeros_like(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for linear layer.\n",
    "        x shape: (N, in_features)\n",
    "        returns shape: (N, out_features)\n",
    "        \"\"\"\n",
    "        self.x = x  # save input\n",
    "        return x.dot(self.weight) + self.bias\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass for linear layer.\n",
    "        dout shape: (N, out_features)\n",
    "        \"\"\"\n",
    "        # Gradient w.r.t. weights and bias\n",
    "        self.grad_weight = self.x.T.dot(dout)\n",
    "        self.grad_bias = np.sum(dout, axis=0)\n",
    "        # Gradient w.r.t. input\n",
    "        dx = dout.dot(self.weight.T)\n",
    "        return dx\n",
    "def softmax_cross_entropy_with_logits(logits, labels):\n",
    "    \"\"\"\n",
    "    Compute softmax probabilities and cross-entropy loss.\n",
    "    logits: (N, C)\n",
    "    labels: (N,) with class indices 0..C-1\n",
    "    Returns: loss (scalar) and softmax probabilities (N, C).\n",
    "    \"\"\"\n",
    "    # Shift logits for numerical stability\n",
    "    logits_shift = logits - np.max(logits, axis=1, keepdims=True)\n",
    "    exp_scores = np.exp(logits_shift)\n",
    "    softmax = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    N = logits.shape[0]\n",
    "    # Cross-entropy loss\n",
    "    correct_logprobs = -np.log(softmax[np.arange(N), labels] + 1e-12)\n",
    "    loss = np.sum(correct_logprobs) / N\n",
    "    return loss, softmax\n",
    "\n",
    "\n",
    "def softmax_grad(softmax, labels):\n",
    "    \"\"\"\n",
    "    Gradient of softmax + cross-entropy loss w.r.t. logits.\n",
    "    \"\"\"\n",
    "    N = softmax.shape[0]\n",
    "    grad = softmax.copy()\n",
    "    grad[np.arange(N), labels] -= 1\n",
    "    grad = grad / N\n",
    "    return grad\n",
    "\n",
    "\n",
    "class SimpleCNN:\n",
    "    \"\"\"\n",
    "    CNN architecture:\n",
    "    Conv(1->16) -> BN -> ReLU -> Pool ->\n",
    "    Conv(16->32) -> BN -> ReLU -> Pool ->\n",
    "    Flatten -> FC(256) -> ReLU -> FC(6)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Layer definitions\n",
    "        self.conv1 = Conv2D(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1   = BatchNorm2D(16)\n",
    "        self.relu1 = ReLU()\n",
    "        self.pool1 = MaxPool2D(2, 2)\n",
    "\n",
    "        self.conv2 = Conv2D(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2   = BatchNorm2D(32)\n",
    "        self.relu2 = ReLU()\n",
    "        self.pool2 = MaxPool2D(2, 2)\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        # Calculate flattened feature size after conv/pool \n",
    "        flat_features = 32 * 37 * 127\n",
    "        self.fc1 = Linear(43808, 256)\n",
    "        self.relu3 = ReLU()\n",
    "        self.fc2 = Linear(256, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through conv -> BN -> ReLU -> Pool blocks\n",
    "        out = self.conv1.forward(x)\n",
    "        out = self.bn1.forward(out)\n",
    "        out = self.relu1.forward(out)\n",
    "        out = self.pool1.forward(out)\n",
    "\n",
    "        out = self.conv2.forward(out)\n",
    "        out = self.bn2.forward(out)\n",
    "        out = self.relu2.forward(out)\n",
    "        out = self.pool2.forward(out)\n",
    "\n",
    "        # Flatten and fully connected layers\n",
    "        out = self.flatten.forward(out)\n",
    "        out = self.fc1.forward(out)\n",
    "        out = self.relu3.forward(out)\n",
    "        out = self.fc2.forward(out)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # Backpropagate through layers in reverse order\n",
    "        dout = self.fc2.backward(dout)\n",
    "        dout = self.relu3.backward(dout)\n",
    "        dout = self.fc1.backward(dout)\n",
    "        dout = self.flatten.backward(dout)\n",
    "\n",
    "        dout = self.pool2.backward(dout)\n",
    "        dout = self.relu2.backward(dout)\n",
    "        dout = self.bn2.backward(dout)\n",
    "        dout = self.conv2.backward(dout)\n",
    "\n",
    "        dout = self.pool1.backward(dout)\n",
    "        dout = self.relu1.backward(dout)\n",
    "        dout = self.bn1.backward(dout)\n",
    "        dout = self.conv1.backward(dout)\n",
    "\n",
    "        return dout\n",
    "\n",
    "    def update_params(self, lr=0.001):\n",
    "        \"\"\"\n",
    "        Update all parameters using SGD with learning rate lr.\n",
    "        \"\"\"\n",
    "        # Conv1 parameters\n",
    "        self.conv1.weight -= lr * self.conv1.grad_weight\n",
    "        self.conv1.bias   -= lr * self.conv1.grad_bias\n",
    "        # BN1 parameters\n",
    "        self.bn1.gamma -= lr * self.bn1.grad_gamma\n",
    "        self.bn1.beta  -= lr * self.bn1.grad_beta\n",
    "        # Conv2 parameters\n",
    "        self.conv2.weight -= lr * self.conv2.grad_weight\n",
    "        self.conv2.bias   -= lr * self.conv2.grad_bias\n",
    "        # BN2 parameters\n",
    "        self.bn2.gamma -= lr * self.bn2.grad_gamma\n",
    "        self.bn2.beta  -= lr * self.bn2.grad_beta\n",
    "        # FC1 parameters\n",
    "        self.fc1.weight -= lr * self.fc1.grad_weight\n",
    "        self.fc1.bias   -= lr * self.fc1.grad_bias\n",
    "        # FC2 parameters\n",
    "        self.fc2.weight -= lr * self.fc2.grad_weight\n",
    "        self.fc2.bias   -= lr * self.fc2.grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T04:55:54.453953Z",
     "iopub.status.busy": "2025-06-01T04:55:54.453368Z",
     "iopub.status.idle": "2025-06-01T05:02:07.132231Z",
     "shell.execute_reply": "2025-06-01T05:02:07.131624Z",
     "shell.execute_reply.started": "2025-06-01T04:55:54.453926Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 597.5251, Train Accuracy: 0.6180, Validation Accuracy: 0.6100\n",
      "Epoch [2/10], Loss: 284.8785, Train Accuracy: 0.7604, Validation Accuracy: 0.8100\n",
      "Epoch [3/10], Loss: 253.9044, Train Accuracy: 0.7857, Validation Accuracy: 0.7600\n",
      "Epoch [4/10], Loss: 228.9225, Train Accuracy: 0.8090, Validation Accuracy: 0.7800\n",
      "Epoch [5/10], Loss: 212.5150, Train Accuracy: 0.8234, Validation Accuracy: 0.7300\n",
      "Epoch [6/10], Loss: 186.9963, Train Accuracy: 0.8425, Validation Accuracy: 0.7800\n",
      "Epoch [7/10], Loss: 163.5944, Train Accuracy: 0.8622, Validation Accuracy: 0.7500\n",
      "Epoch [8/10], Loss: 146.3313, Train Accuracy: 0.8809, Validation Accuracy: 0.7800\n",
      "Epoch [9/10], Loss: 125.9772, Train Accuracy: 0.8974, Validation Accuracy: 0.7700\n",
      "Epoch [10/10], Loss: 111.6245, Train Accuracy: 0.9057, Validation Accuracy: 0.7600\n"
     ]
    }
   ],
   "source": [
    "net = SimpleCNN()\n",
    "\n",
    "# Hiperparámetros\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.numpy(), labels.numpy()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = net.forward(images)\n",
    "        loss, probs = softmax_cross_entropy_with_logits(logits, labels)\n",
    "        running_loss += loss\n",
    "\n",
    "        # Cálculo de precisión\n",
    "        predictions = np.argmax(probs, axis=1)\n",
    "        correct_train += np.sum(predictions == labels)\n",
    "        total_train += labels.shape[0]\n",
    "\n",
    "        # Backward pass\n",
    "        grad_logits = softmax_grad(probs, labels)\n",
    "        net.backward(grad_logits)\n",
    "\n",
    "        # Actualización de parámetros\n",
    "        net.update_params(learning_rate)\n",
    "\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    # Validación del modelo\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.numpy(), labels.numpy()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = net.forward(images)\n",
    "        loss, probs = softmax_cross_entropy_with_logits(logits, labels)\n",
    "        val_loss += loss\n",
    "\n",
    "        # Cálculo de precisión\n",
    "        predictions = np.argmax(probs, axis=1)\n",
    "        correct_val += np.sum(predictions == labels)\n",
    "        total_val += labels.shape[0]\n",
    "    \n",
    "    val_accuracy = correct_val / total_val\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 111880,
     "sourceId": 269359,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 224843881,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 73.034129,
   "end_time": "2025-02-12T16:08:13.533651",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-12T16:07:00.499522",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
